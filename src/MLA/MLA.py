# -*- coding: utf-8 -*-
"""教材_MLA_1.5.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fgaXWID5294hJMPpqj1Wd2ErE96lEEqj
"""

import os
import io
import json
import math
import time
import importlib.util
from pathlib import Path

import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import DataLoader

!rm -R DeepSeekCode
!git clone https://github.com/Michi-123/DeepSeekCode.git

from DeepSeekCode import KVCache, precompute_freqs_cis, apply_rope, create_causal_mask
from DeepSeekCode.src.MLA.Transformer import Transformer, RMSNorm, ScaledDotProductAttention
from DeepSeekCode.src.MLA.Test import test
from DeepSeekCode.util.data.deepseek_dataset import AozoraCharLevelDataset
import DeepSeekCode.util.functions as fn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#@title 引数のクラスの定義
class Args:
    def __init__(self, d_model, n_heads, d_head, d_rope, d_c, d_cQ, max_seq_len, rope_theta, context_size, n_layers, vocab_size):

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_rope = d_rope
        self.d_c = d_c
        self.d_cQ = d_cQ
        self.max_seq_len = max_seq_len
        self.rope_theta = rope_theta
        self.context_size = context_size
        self.n_layers = n_layers
        self.vocab_size = vocab_size

# @title MLA
class MLA(nn.Module):
    """
    Multi-Head Latent Attention (MLA)

    複数の射影空間（latent Q/K/V）を用いて効率的に情報を抽出するTransformerのアテンションブロック。
    本クラスでは KV キャッシュを外部クラス (KVCache) に分離して再利用可能にした構造になっている。
    """

    def __init__(self, args):
        super().__init__()

        """ 引数の設定 """
        # モデル全体の隠れ状態次元
        self.d_model = args.d_model
        # Attention ヘッド数
        self.n_heads = args.n_heads
        # latent空間の次元数
        self.d_cQ = args.d_cQ  # 10%～30%の圧縮
        self.d_c = args.d_c  # 5%～10%の圧縮
        # multi-head dimension
        self.d_h = args.d_head
        self.d_hR = args.d_rope # ヘッドの次元の25%～50%
        # KVキャッシュの最大長
        self.context_size = args.context_size

        # Q, K, V の射影層（入力 -> latent表現）

        """ Queery """

        # 入力 -> latent Q
        self.q_down_proj = nn.Linear(self.d_model, self.d_cQ)

        # latent Q に対する正規化
        self.q_norm = RMSNorm(self.d_cQ)

        # latent Q -> 再構成された Qc / Qr（2つの役割で別々に使う）
        self.qc_up_proj = nn.Linear(self.d_cQ, self.n_heads * self.d_h)
        self.qr_up_proj = nn.Linear(self.d_cQ, self.n_heads * self.d_hR)

        """ Key for RoPE """

        # 入力から直接得られる Kr（回転対象）
        self.kr_proj = nn.Linear(self.d_model, self.d_hR)
        self.kr_norm = RMSNorm(self.d_hR)

        """ Key / Value  """

        # 入力 -> latent K/V
        self.kv_down_proj = nn.Linear(self.d_model, self.d_c)
        # 正則化
        self.kv_norm = RMSNorm(self.d_c)
        # latent K/V から再構成された Kc, Vc
        self.kc_up_proj = nn.Linear(self.d_c, self.n_heads * self.d_h)
        self.vc_up_proj = nn.Linear(self.d_c, self.n_heads * self.d_h)

        # KVキャッシュ（初期化は forward 時に行う）
        self.kv_cache = None


        """ Attention """

        # 注意計算モジュール（scaled dot-product）
        self.attention = ScaledDotProductAttention(self.d_model)
        # 注意計算の出力用の線形変換
        self.output_head = nn.Linear(self.n_heads * self.d_h, self.d_model)


    def reset_kv_cache(self):
        """
        KVキャッシュを明示的にリセットする関数。
        """
        if self.kv_cache:
            self.kv_cache.reset()

    def forward(self, h, freqs_cis, mask=None, train=False):
        """
        Args:
            h: 入力テンソル (batch_size, seq_len, d_model)
            freqs_cis: RoPEで使う複素周波数埋め込み
            mask: Attentionマスク
            train: 訓練中かどうか（Falseの場合のみキャッシュを使う）
        """
        batch_size, seq_len, _ = h.shape

        # 最初の推論ではキャッシュを初期化
        if self.kv_cache is None:
            self.kv_cache = KVCache(self.context_size)

        # --- Query 処理 ---
        # 入力 -> latent Q
        cQ = self.q_down_proj(h)
        cQ = self.q_norm(cQ)
        # latent Q -> 回転用 Qr
        qR = self.qr_up_proj(cQ)
        qR = qR.reshape(batch_size, seq_len, self.n_heads, self.d_hR)
        # RoPEによる回転埋め込みをqRにのみ適用
        qR = apply_rope(qR, freqs_cis)
        # latent Q -> 通常の Qc
        qC = self.qc_up_proj(cQ)
        qC = qC.reshape(batch_size, seq_len, self.n_heads, self.d_h)

        # 結合された Q（最終的なAttention用）
        q = torch.cat([qC, qR], dim=-1)

        # --- Key-Value 処理 ---
        # Kr（入力から直接）
        kR = self.kr_proj(h)
        kR = self.kr_norm(kR)
        kR = kR.reshape(batch_size, seq_len, 1, self.d_hR) # kRは1ヘッド

        # RoPEを適用をkRにのみ適用
        kR = apply_rope(kR, freqs_cis)

        # latent key/value
        cKV = self.kv_down_proj(h)
        cKV = self.kv_norm(cKV)

        if not train:
            # 推論時のみキャッシュに保存し、全過去トークンと照合
            self.kv_cache.update(kR, cKV)
            kR, cKV = self.kv_cache.get()

        # cKV -> kC, vC（Attention用の形式に再構成）
        kC = self.kc_up_proj(cKV)
        vC = self.vc_up_proj(cKV)
        kC = kC.reshape(batch_size, -1, self.n_heads, self.d_h)
        vC = vC.reshape(batch_size, -1, self.n_heads, self.d_h)

        # kR: 回転的、kC: 内容的 Key
        # ここでkRをブロードキャスト
        kR = kR.expand(-1, -1, kC.size(2), -1)

        k = torch.cat([kR, kC], dim=-1)

        # V は通常通り latent V -> Vc
        v = vC

        # --- Attention ---

        q = q.transpose(1, 2) # (B, T, H, D) → (B, H, T, D)
        # q = q.permute(0, 2, 1, 3)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Sacled Dot-Product Attetnionの計算
        h, attn_weight = self.attention(q, k, v, mask)

        # 出力を整形して返す
        h = h.transpose(1, 2)
        h = h.reshape(batch_size, seq_len, self.n_heads * self.d_h) # H*Dでトークンのベクトルに変換
        h = self.output_head(h)

        output = {}
        output['hidden_state'] = h
        output['attn_weight'] = attn_weight

        return output

#@title MHA
class MHA(nn.Module):
    """
    Multi-Head Attention with RoPE + KVCache support.

    変更点（要約）:
      - KVキャッシュ（self.kv_cache）を追加し、推論時（train=False）のみ使用。
      - RoPE適用後の k, v を (B, S, H, D) 形状のまま KVCache に蓄積 → 取得。
      - 取得後に (B, H, S, D) に転置して Attention を実行。
      - reset_kv_cache() を実装（生成前に各層で呼び出し可能）。
    前提:
      - Args に context_size が存在すること（既存ファイルと同一）。
      - apply_rope(), ScaledDotProductAttention は既存のものを使用。
    """
    def __init__(self, args):
        super().__init__()
        self.n_heads = args.n_heads
        self.d_model = args.d_model
        self.d_head = args.d_head

        d_model = args.d_model
        # MLA比較用の出力次元（既存実装に合わせる）
        self.fc_q = nn.Linear(d_model, self.n_heads * self.d_head)
        self.fc_k = nn.Linear(d_model, self.n_heads * self.d_head)
        self.fc_v = nn.Linear(d_model, self.n_heads * self.d_head)

        dropout = 0.1
        self.attention = ScaledDotProductAttention(d_model, dropout)
        self.fc = nn.Linear(self.n_heads * self.d_head, d_model)
        self.dropout = nn.Dropout(dropout)

        # Xavier init（既存実装に合わせる）
        nn.init.xavier_uniform_(self.fc_q.weight)
        nn.init.xavier_uniform_(self.fc_k.weight)
        nn.init.xavier_uniform_(self.fc_v.weight)
        nn.init.xavier_uniform_(self.fc.weight)

        # --- KVCache 追加 ---
        self.context_size = args.context_size
        self.kv_cache = None
        self.registered_batch_size = None

    def reset_kv_cache(self):
        """外部から明示的にキャッシュをクリアするための関数。"""
        if self.kv_cache:
            self.kv_cache.reset()
        self.registered_batch_size = None

    def forward(self, x, freqs_cis, mask=None, train=False):
        """
        x: (N, S, d_model)
        freqs_cis: RoPE 用の複素周波数（precompute_freqs_cis の出力）
        mask: (S, S) など（学習時のみ使用想定）。
        train: 学習時 True / 推論時 False（False のとき KVCache 使用）。
        """
        N, S = x.size(0), x.size(1)
        H = self.n_heads
        D = self.d_head

        # 初期化（バッチサイズが変わった場合などもここで再生成して良い）
        if (not train) and (self.kv_cache is None):
            self.kv_cache = KVCache(self.context_size)

        # 1) Q, K, V を作成
        q = self.fc_q(x)
        k = self.fc_k(x)
        v = self.fc_v(x)

        # 2) (N, S, H, D) へ整形
        q = q.view(N, S, H, D)
        k = k.view(N, S, H, D)
        v = v.view(N, S, H, D)

        # 3) RoPE を Q, K に適用
        #    freqs: (S, D//2) を切り出し、apply_rope は (N, S, H, D) を想定
        freqs = freqs_cis[:S, : D // 2].to(q.device)
        q = apply_rope(q, freqs)
        k = apply_rope(k, freqs)

        # 4) 推論時のみ KVCache を使用
        if not train:
            # （注意）KVCache は seq 次元(dim=1)で結合する設計
            # ここでは (N, S, H, D) 形状のまま渡す
            self.kv_cache.update(k, v)
            k, v = self.kv_cache.get()  # k, v: (N, S_total, H, D)

        # 5) Attention へ（ScaledDotProductAttention は (N, H, T, D) を想定）
        q = q.transpose(1, 2)  # (N, H, S_q, D)
        k = k.transpose(1, 2)  # (N, H, S_k, D)
        v = v.transpose(1, 2)  # (N, H, S_k, D)

        # 生成時（seq=1）に KVCache を使う場合、mask は (1,1) で問題なし。
        # 学習時は通常の (S,S) マスクをそのまま渡す。
        x, attn_weight = self.attention(q, k, v, mask=mask)

        # 6) ヘッド結合 & 出力
        x = x.transpose(1, 2).contiguous().view(N, -1, H * D)
        x = self.fc(x)
        x = self.dropout(x)
        output = {}
        output["hidden_state"] = x
        output["attn_weight"] = attn_weight
        return output

# !pip install -qq datasets==2.18

# @title 青空文庫データセットの利用
if 0:
    from datasets import load_dataset
    ds = load_dataset('globis-university/aozorabunko-clean')
    ds = ds.filter(lambda row: row['meta']['文字遣い種別'] == '新字新仮名' )

# @title 青空文庫の学習データの取得
if 0:

    authors = ['芥川'] #@param
    # authors = ['夏目'] #@param
    titles =["松江印象記"]#@param
    # titles = [ 'こころ', '三四郎' ,'それから', '坊っちゃん']#@param
    # titles = ['文鳥', '正岡子規', '夢十夜']#@param
    corpus = ''
    for book in ds['train']:
        title = book['meta']['作品名']
        author = book['meta']['姓']
        # if author in authors:
        if title in titles and author in authors:
            print(author, title)
            text = book['text'] # 本文
            text = ''.join(text.split()) # Clean up
            # text = tagger.parse(text) # 形態素解析
            corpus += text

    print(corpus[:100])

# @title 作品タイトルをキーにした辞書型
urls = {
    # "mikan":    "https://www.aozora.gr.jp/cards/000879/files/43017_17431.html",  # 蜜柑
    "matsue":   "https://www.aozora.gr.jp/cards/000879/files/96_15248.html",    # 松江印象記
    # "trokko":   "https://www.aozora.gr.jp/cards/000879/files/43016_16836.html", # トロッコ
    # "imogayu":  "https://www.aozora.gr.jp/cards/000879/files/55_14824.html",    # 芋粥
    # "jigokuhen":"https://www.aozora.gr.jp/cards/000879/files/60_15129.html",    # 地獄変
}

corpus = fn.get_aozora_corpus(urls)
corpus

# @title 学習のパラメータ
batch_size   = 64
num_epochs   = 20 # 40
lr           = 0.01

use_MLA = False #@param{type:"boolean"}

# @title モデルのハイパーパラメータ
d_model = 128
n_heads = 4

if use_MLA:
    # MLA
    d_c    = 16
    d_cQ   = 32 # > d_c
    d_rope = 16
    d_head = 48

else:
    # MHA
    d_rope = d_model // n_heads
    d_head = d_model // n_heads
    d_c = None
    d_cQ = None

n_layers = 1
max_seq_len  = 1000         # also used by RoPE precompute
context_size = 20        # training sequence length (<= max_seq_len)
rope_theta   = 10000.0

#@title インスタンスの作成
args = Args(
    d_model=d_model,
    n_heads=n_heads,
    d_head=d_head,
    d_rope=d_rope,
    d_c=d_c,
    d_cQ=d_cQ,
    max_seq_len=max_seq_len,
    rope_theta=rope_theta,
    context_size=context_size,
    n_layers=n_layers,
    vocab_size=None
)

args.n_layers = n_layers
args.lr = lr
args.batch_size = batch_size
args.num_epochs = num_epochs

torch.manual_seed(42)

# Precompute RoPE coefficients once (max_seq_len x head_dim) on device
freqs_cis = precompute_freqs_cis(args, device=device)

train_dataset = AozoraCharLevelDataset(corpus, context_size)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)

args.vocab_size = train_dataset.vocab_size

if use_MLA:
    model = Transformer(args, MLA).to(device)
else:
    model = Transformer(args, MHA).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# 評価指標の設定
criterion = nn.CrossEntropyLoss() # 合格ラインを判定するための計算

start_epoch = 0

"""# 学習"""

# @title Train
from tqdm import tqdm

losses_list = []
for epoch in range(start_epoch, num_epochs + 1):

    model.train()
    total_loss = 0.0
    total_tokens = 0

    i = 0

    for  x, y in tqdm(train_dataloader):

        x = x.to(device)  # (B, T)
        y = y.to(device)  # (B, T)
        T = x.size(1)

        # Create causal mask for sequence length T
        mask = create_causal_mask(T).to(device)  # shape (T, T) or (1, T, T) depending on implementation

        input_freqs_cis = freqs_cis[:T]

        # Forward
        logits = model(x, freqs_cis=input_freqs_cis, mask=mask, train=True)  # (B, T, V)
        loss = criterion(logits.reshape(-1, args.vocab_size), y.reshape(-1))

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_loss += loss.item()

        i += 1

    train_loss = total_loss / i

    print()
    print(f"Epoch {epoch:02d} | loss: {train_loss:.4f}")
    losses_list.append(train_loss)

    start_epoch = epoch

# @title 訓練状態も含めて保存
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    # 'accuracy': accuracy
}

torch.save(checkpoint, 'checkpoint.pth')

# チェックポイントから復元
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
epoch

fn.log_run(args, losses_list, use_MLA)

fn.show_parameters()

fn.show_loss_graph(losses_list)

"""# Test"""

text = "松江へ来て、まず" #@param{type:"string"}
generate_length = 500 #@param{type:"integer"}
test(model, text, freqs_cis, train_dataset.stoi, train_dataset.itos, max_seq_len=generate_length)

"""https://www.aozora.gr.jp/cards/000879/files/96_15248.html

<table>
<tr>
  <td>DeepSeek</td><td>128,000</td>
</tr>
<tr>
  <td>GPT-4o</td><td>128,000</td>
</tr>
<tr>
  <td>GPT-5</td><td>400,000</td>
</tr>
</table>

MLAがMHAよりも精度が悪いのは、少量のコーパスでも言語の特徴が多すぎるためです。MHAはでうまくいっているのは、考えることなく覚えたままを予測（Over fitting）しているだけです。大規模コーパスを扱って初めて圧縮の意味が出てくるのではないかと思います。
そういった意味で、この教材のMLAは小規模ながらも、「オウム返し」という行為から「考える」という行為を初めから学習しようとしていると言えます。
"""

