# -*- coding: utf-8 -*-
"""Tokenizer_0.2.3_use_eos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fTczVuKCOWyqG3xXRQd0loO7gnROhnGX

Tokenizer_0.2.2 特殊トークンの並びを修正
"""

import pickle
import re
import torch

# @title Tokenizer
class Tokenizer:
    def __init__(self, text='', tagger=None):

        # 最初の設定
        self.tagger = tagger

        # 初期化の処理
        self.initialize(text)

# @title initialize
class Tokenizer(Tokenizer):
    def _old_initialize(self, text=''):
        # 分割
        text_parsed = self.parse(text) # Renamed to avoid confusion with the `vocab` variable

        # 並べ替え
        # Use a set to ensure uniqueness before sorting
        vocab =  sorted(list(set(text_parsed.split())))

        # 特殊文字の追加
        # Pass a copy of the vocab to _add_special_tokens to avoid modifying the original set order prematurely
        self._add_special_tokens(vocab) # Modified to take `vocab` as a list

        # self.max_size = 5000 必要であれば追加
        # Ensure that word2index and index2word are built from a unique list of words
        # Re-sort vocab to ensure consistent indexing after adding special tokens
        final_vocab = sorted(list(set(vocab))) # Convert to set for uniqueness, then back to list and sort

        self.index2word = {idx: word for idx, word in enumerate(final_vocab)}
        self.word2index = {word: idx for idx, word in enumerate(final_vocab)}

        # self.vocab_size = len(self.word2index) + 1  これでindexエラー？
        self.vocab_size = len(self.word2index)
        self.word_freq = {}
        self.word_freq_desc = {}

        # self.max_length = max_length

        self.pad_token_id = self.word2index['<pad>']  # 一般的にパディングは0。必要なら変更可。
        self.eos_token_id = self.word2index['<eos>']

        # 頻出度を更新
        self._create_word_freq(text_parsed) # Use the parsed text here

        self.nodel_name = None

# @title initialize
class Tokenizer(Tokenizer):

    def initialize(self, text=''):
        text_parsed = self.parse(text)
        words = list(set(text_parsed.split()))  # 一意な単語

        special_tokens = [
            '<pad>','<bos>','<eos>','<unk>',
            '<think>','</think>',
            '<answer>', '</answer>',
            '<ext1>', '<ext2>'
        ]

        # 特殊トークンを順番そのままで先頭に追加
        final_vocab = []
        for token in special_tokens:
            if token not in final_vocab:
                final_vocab.append(token)

        # 特殊トークンを除いた語彙をアルファベット順で追加
        normal_tokens = sorted([w for w in words if w not in special_tokens])
        final_vocab.extend(normal_tokens)

        self.index2word = {idx: word for idx, word in enumerate(final_vocab)}
        self.word2index = {word: idx for idx, word in enumerate(final_vocab)}
        self.vocab_size = len(self.word2index)

        self.pad_token_id = self.word2index['<pad>']
        self.eos_token_id = self.word2index['<eos>']
        self._create_word_freq(text_parsed)

# @title _add_special_tokens
class Tokenizer(Tokenizer):
    def _add_special_tokens(self, vocab):
        # Insert special tokens at the beginning.
        # This method is modified to modify the passed `vocab` list in place.
        special_tokens = [
            '<pad>','<bos>','<eos>','<unk>',
            '<think>','</think>',
            '<answer>', '</answer>',
            '<ext1>', '<ext2>']
        for token in reversed(special_tokens):
            if token not in vocab: # Only add if not already present
                vocab.insert(0, token)

# @title parse
class Tokenizer(Tokenizer):
    def parse(self, text):
        # 小文字変換
        text = text.lower()

        # Mecabでパース処理
        if self.tagger is not None:
            text = self.tagger.parse(text)
        else:
            # 文字単位でトークン化
            text = ' '.join(list(text))
            # print('tagger is None')
            pass

        # 記号を分割
        text = self._split_symbols(text)
        # 数値を分割
        text = self._split_digits(text)
        # 空白の補正
        text = text.split()  # 空白で分割し、連続する空白はまとめて1つの区切りとみなされる
        text = " ".join(text) # 分割された単語を1つの空白で結合
        # 特殊記号の補正
        text = self._adjust_special_tokens(text)

        return text

# @title load
class Tokenizer(Tokenizer):
    def load(self, tokenizer_path):
        """
        Open pickel file and load vocab
        """
        with open(tokenizer_path, 'rb') as f:
            vocab = pickle.load(f)

        self.index2word = vocab.index2word
        self.word2index = vocab.word2index
        self.vocab_size = len(self.word2index)

    def from_pretrained(self, tokenizer_path):
        print("このメソッドは非推奨です。代わりに `load` を使用してください。")
        self.load(tokenizer_path)

# @title save
class Tokenizer(Tokenizer):
    def save(self, tokenizer_path):
        tokenizer.tagger = None # MeCabを保存できないので削除
        import pickle
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(tokenizer, f)

# @title _split_symbols
class Tokenizer(Tokenizer):
    def _split_symbols(self, text):
        # 記号を区切る
        # return re.sub(r'\s+', ' ', re.sub(r'([^a-zA-Z0-9\s])', r' \1 ', text))
        # return re.sub(r'([^a-zA-Z0-9\s])', r' \1 ', text)
        return re.sub(r'([!"#$%&\'()*+,\-./:;<=>?@[\\\]^_`{|}~])', r' \1 ', text).strip()

# @title _split_digits
class Tokenizer(Tokenizer):
    def _split_digits(self, text):
        # 数字部分を1桁ずつスペース区切りに変換
        return re.sub(r'\d+', lambda m: ' '.join(m.group()), text)

# @title _adjust_special_tokens
class Tokenizer(Tokenizer):
    def _adjust_special_tokens(self, text):
        # 特殊文字の補正
        text = text.replace('< eos >', '<eos>')
        text = text.replace('< pad >', '<pad>')

        text = text.replace('< think >', '<think>')
        text = text.replace('< / think >', '</think>')
        text = text.replace('< answer >', '<answer>')
        text = text.replace('< / answer >', '</answer>')

        text = text.replace('< e o s >', '<eos>')
        text = text.replace('< p a d >', '<pad>')

        text = text.replace('< t h i n k >', '<think>')
        text = text.replace('< / t h i n k >', '</think>')
        text = text.replace('< a n s w e r >', '<answer>')
        text = text.replace('< / a n s w e r >', '</answer>')

        return text

class Tokenizer(Tokenizer): # 既存のTokenizerクラスを継承していると仮定
    def _split_digits(self, text):
        # 数字とその他の文字の間にスペースを挿入し、数字は1桁ずつスペース区切りにする
        # 例: "abc123def" -> "a b c 1 2 3 d e f"
        # 例: "123abc45" -> "1 2 3 a b c 4 5"

        # 数字部分を1桁ずつスペース区切りに変換
        text = re.sub(r'\d+', lambda m: ' '.join(m.group()), text)

        # 数字とそれ以外の文字の間にスペースを挿入
        # ただし、すでにスペースで区切られている場合は追加しない
        text = re.sub(r'(\D)(\d)', r'\1 \2', text) # 非数字の後に数字が続く場合
        text = re.sub(r'(\d)(\D)', r'\1 \2', text) # 数字の後に非数字が続く場合

        # 連続するスペースを1つにまとめる
        text = re.sub(r'\s+', ' ', text).strip()

        return text

# 使用例
# tokenizer = Tokenizer()
# print(tokenizer._split_digits("abc123def"))
# print(tokenizer._split_digits("123abc45"))
# print(tokenizer._split_digits("あいう123えお"))
# print(tokenizer._split_digits("123 456"))
# print(tokenizer._split_digits("test_1.0"))

# @title encode
class Tokenizer(Tokenizer):
    def encode(self, text, max_length=None, padding=None
            # truncation=True,
            # return_tensors="pt"
             ):

        text = self.parse(text)

        ids = []
        for word in text.split():
            try:
                index = self.word2index[word]
            except:
                index = self.word2index['<unk>'] # 未登録の単語として処理
            ids.append(index)

        if padding is not None:
            ids = self.padding(ids, padding)

        if max_length is not None:
            ids = ids[:max_length]

        ids = torch.LongTensor(ids)
        return ids

# @title decode
class Tokenizer(Tokenizer):
    def decode(self, indices, use_eos=False):

        if indices.ndim == 0:
            indices = indices.unsqueeze(0)

        elif indices.ndim == 2:
            indices = indices.squeeze()

        indices = indices.tolist()

        text = ''
        for index in indices:
            word = self.index2word[index]

            if use_eos:
                if word == self.eos_token_id:
                    break

            text += word

        return text

# @title padding
class Tokenizer(Tokenizer):
    def add_padding(self, indices, max_length):
        pad_len = max_length - len(indices)
        if pad_len > 0:
            indices += [self.pad_token_id] * pad_len
        return indices

    def padding(self, indices, max_length):
        print("★===このメソッドは非推奨です===")
        return self.add_padding(indices, max_length)

# @title
class Tokenizer(Tokenizer):
    def _create_word_freq(self, corpus):

        word_freq = {}

        """ 新しいコーパスから単語の頻度を更新 """
        for word in corpus.split():
            if word not in word_freq.keys():
                word_freq[word] = 1
            else:
                word_freq[word] += 1

        # 頻出度でソート
        self.word_freq = dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=False))
        self.word_freq_desc = dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True))

"""# TEST"""

if __name__ == '__main__':
    text = "10たす21は?<think>10+21=21</think><answer>21</answer><EOS>123"

    tokenizer = Tokenizer('text')

    text = tokenizer._split_symbols(text)
    print(text)

    text = tokenizer._split_digits(text)
    print(text)


    text = text.split()  # 空白で分割し、連続する空白はまとめて1つの区切りとみなされる
    text = " ".join(text) # 分割された単語を1つの空白で結合
    print(text)

    text = tokenizer._adjust_special_tokens(text)
    print(text)
    print()


    text = tokenizer.parse(text)
    print(text)

# vocab size
if __name__ == '__main__':
    text = "10たす21は?<think>10+21=21</think><answer>21</answer><EOS>123"
    tokenizer = Tokenizer('a b c d apple')
    tokenizer.initialize(text)
    print(tokenizer.vocab_size)
    print(tokenizer.word2index)
    print(tokenizer.index2word[19])

if __name__ == '__main__':
    tokenizer = Tokenizer()
    print(tokenizer.index2word)

if __name__ == '__main__':
    text = "<BOS>10たす21は?<think>10+21=31</think><answer>31</answer><EOS>"
    tokenizer = Tokenizer(text)
    ids = tokenizer.encode(text)
    generated_text = tokenizer.decode(ids)
    print(generated_text)

if __name__ == '__main__':
    # tokenizer.parse(text)
    text = "<BOS>10たす21は?<think>10+21=31</think><answer>31</answer><EOS>"
    # 小文字変換
    text = text.lower()

    # Mecabでパース処理
    if tokenizer.tagger is not None:
        text = tokenizer.tagger.parse(text)
    else:
        # 文字単位でトークン化
        text = ' '.join(list(text))
        # print('tagger is None')
        pass

    # 記号を分割
    text = tokenizer._split_symbols(text)
    # 数値を分割
    text = tokenizer._split_digits(text)
    # 空白の補正
    text = text.split()  # 空白で分割し、連続する空白はまとめて1つの区切りとみなされる
    text = " ".join(text) # 分割された単語を1つの空白で結合
    # 特殊記号の補正
    text = tokenizer._adjust_special_tokens(text)
    print(text)

if __name__ == '__main__':
    # MeCabの利用 (日本語を形態素解析するプログラム)
    pass

    # # MeCabのインストール
    # !pip install -qq datasets==3.2.0
    # !pip install -qq mecab-python3==1.0.8
    # !pip install -qq unidic-lite
    # import MeCab

    # tagger = MeCab.Tagger("-Owakati")
    # print(tagger.parse('私は昨日公園に行きました。'))
    # EOS = '<EOS>' # End of sentence.
    # SPACE = ' '