# -*- coding: utf-8 -*-
"""checkpoint_0.1.0_add_args.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xYeQ9tYhugsBkXVkKxoMSdj0hMqDqGPA

checkpoint
"""

import torch

# @title save_checkpoint: モデルとオプティマイザの保存
def save_checkpoint(args, model, optimizer, epoch, losses, checkpoint_path):
    try:
        checkpoint = {
            'args': args,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': epoch,
            'losses': losses,
        }
        torch.save(checkpoint, checkpoint_path)

    except Exception as e:
        print(f"チェックポイントの保存中にエラーが発生しました: {e}")

# @title load_checkpoint:
def load_checkpoint(args, model, checkpoint_path, optimizer=None):
    epoch = None
    losses = None

    # チェックポイントをロード
    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=False) # Default がTrueなので明示的にFalseを指定
    args = checkpoint['args']

    # モデルの重みをロード
    model.load_state_dict(checkpoint['model_state_dict'])

    # オプティマイザの状態をロード
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    # 追加情報
    epoch = checkpoint.get('epoch')
    losses = checkpoint.get('losses')
    if epoch is not None:
        print('epoch', epoch)

    # 必要に応じて、ロード後の学習率を調整することもできます
    # for param_group in optimizer.param_groups:
    #     param_group['lr'] = 0.0001

    return epoch, losses

